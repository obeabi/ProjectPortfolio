{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FlightCancellation_project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_XiUtgWGIOL4"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/obeabi/ProjectPortfolio/blob/master/FlightCancellation_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW-CD9qPUXiR"
      },
      "source": [
        "# Written by Abiola Obembe\n",
        "# SDS Challenge #1 - Flight Cancellations\n",
        "## 2020-10-30\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaT4o-v0Up4V"
      },
      "source": [
        "## Problem Statement\n",
        "I have been hired by the US Department of Transportation (DOT) to analyze data from multiple airline carriers in the United States. The DOT wants to help airline carriers reduce the number of flight cancellations and improve travelers' experiences. My job is to help the DOT predict whether or not a flight will be canceled based on the data provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvyy8tV7VEZ5"
      },
      "source": [
        "### Step 1: Data Preprocessing Tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsUbaYb6VCZ_",
        "outputId": "10655a0e-8ca5-445b-cf2c-1ce9727a7a30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "!pip install imbalanced-learn\n",
        "!pip install category_encoders\n",
        "import category_encoders as ce\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "print('Libraries installed successfully!')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.6/dist-packages (0.4.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->imbalanced-learn) (0.17.0)\n",
            "Requirement already satisfied: category_encoders in /usr/local/lib/python3.6/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.22.2.post1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.10.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.18.5)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n",
            "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.1.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (0.17.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2.8.1)\n",
            "Libraries installed successfully!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAN_VFooV8lb"
      },
      "source": [
        "# Importing the dataset and check the shape and total number of missing values\n",
        "\n",
        "df_train = pd.read_csv('public_flights.csv')\n",
        "\n",
        "df_train.head()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B11R3A23b8os"
      },
      "source": [
        "# print target column name as a list\n",
        "target_column = [df_train.columns[-1]]\n",
        "\n",
        "print(target_column)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW8EYUGAoUyz"
      },
      "source": [
        "# Examine target column for unbalanced data\n",
        "df_train[target_column].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8P_OQlmKFLx"
      },
      "source": [
        "count_classes = pd.value_counts(df_train['CANCELLED'], sort = True)\n",
        "count_classes.plot(kind = 'bar', rot = 0)\n",
        "plt.title(\"Cancelled Flights Distribution\")\n",
        "plt.xticks(range(2))\n",
        "plt.xlabel(\"CANCELLED\")\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfjbSepuMJ0Q"
      },
      "source": [
        "# gET THE CANCELLED AND NOT CANCEELED FLIFGTS\n",
        "Cancelled = df_train[df_train['CANCELLED']== 1]\n",
        "notCancelled = df_train[df_train['CANCELLED']== 0]\n",
        "\n",
        "print(Cancelled.shape, notCancelled.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy0G7oUKh0wV"
      },
      "source": [
        "# Importing the train-set nd check the shape and total number of missing values\n",
        "\n",
        "print(\"The shape of the train-set is:\", (df_train.shape))\n",
        "print(\"The number of rows in the train-set is:\", str(df_train.shape[0]))\n",
        "print(\"The number of columns in the train-set is:\", str(df_train.shape[1]))\n",
        "\n",
        "missing_valuestrain = df_train.isnull().sum().sum()\n",
        "\n",
        "print(\"The number of missing values in the train-set is:\", str(missing_valuestrain))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCbWRNp_n7RS"
      },
      "source": [
        "# create dataframe for test test\n",
        "df_test = pd.read_csv('pred_flights.csv')\n",
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYQNQY7on7s3"
      },
      "source": [
        "# test set dataframe is missing headers information so we will re-initailize it, deleting the target column label\n",
        "labels = df_train.columns\n",
        "\n",
        "new_label= labels[:-1]  # select all column labels except the target column\n",
        "\n",
        "#new_label\n",
        "df_test = pd.read_csv('pred_flights.csv', header =None, names = new_label)\n",
        "\n",
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dGjL3dCoEq0"
      },
      "source": [
        "# Importing the test-set and check the shape and total number of missing values\n",
        "\n",
        "print(\"The shape of the test-set is:\", (df_test.shape))\n",
        "print(\"The number of rows in the test-set is:\", str(df_test.shape[0]))\n",
        "print(\"The number of columns in the test-set is:\", str(df_test.shape[1]))\n",
        "\n",
        "missing_valuestest = df_test.isnull().sum().sum()\n",
        "\n",
        "print(\"The number of missing values in the test-set is:\", str(missing_valuestest))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDQCCXw1oIWW"
      },
      "source": [
        "### Step 2: Feature Engineering (Training and Test Set )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EuLlcgtk9IV"
      },
      "source": [
        "# Examine the data types for train dataframe\n",
        "df_train.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veARPOxffHyz"
      },
      "source": [
        "# Check for training set data frame info\n",
        "df_train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiFcG975h-XS"
      },
      "source": [
        "# Dealing with missing values in the train-set\n",
        "\n",
        "df_train.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v47C0PRloUg1"
      },
      "source": [
        "#### (A) Dealing with Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbLaCRECJivv"
      },
      "source": [
        "# Remove rows with missing target, separate target from predictors\n",
        "X_full = df_train.copy()\n",
        "X_full.dropna(axis=0, subset=['CANCELLED'], inplace=True)\n",
        "y = X_full.CANCELLED\n",
        "X_full.drop(['CANCELLED'], axis=1, inplace=True)\n",
        "\n",
        "# Show X_full dataframe representing predictors only\n",
        "X_full.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcl3pplmKY1L"
      },
      "source": [
        "# Print first 10 entries of target column\n",
        "y[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WtlGwLWKpHk"
      },
      "source": [
        "# Break off validation set from training data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y,train_size=0.8, test_size=0.2, random_state=0)\n",
        "\n",
        "# Organize test set\n",
        "X_test_full = df_test.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJkFXT9IL1O1"
      },
      "source": [
        "# Select categorical columns from X_train_full\n",
        "# All categorical columns\n",
        "category_cols = [col for col in X_train_full.columns if X_train_full[col].dtype == \"object\"]\n",
        "category_cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RbfVEU1NzR0"
      },
      "source": [
        "# Print object caregories for X_train_full for inspection\n",
        "X_train_full[category_cols].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9I3kH0uMH_n"
      },
      "source": [
        "# Let us see the cardanilatity of each column for the categorical columns in X_train_full and X_valid_full\n",
        "# Unique value sin each columns\n",
        "for cols in category_cols:\n",
        "    print(\"Unique values in\", cols,  \"column in training data:\", X_train_full[cols].nunique())\n",
        "    print(\"\\nUnique values in\" , cols,  \"column in validation data:\", X_valid_full[cols].nunique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMIsFnBAhNAt"
      },
      "source": [
        "##### We observe that the cardinality of the columns are all greater than 10 hence one-hot encoding will lead to a huge number of new columns and hence result in the curse of dimensionality problem. Hence in this project we proceed with label encoding for the baseline project. We hope to improve the categorical encoding columns with the category encoder library in the future using the count encoder, target encoder and catboost encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxH9EeOZO30A"
      },
      "source": [
        "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
        "low_cardinality = [cname for cname in X_train_full.columns if\n",
        "                    X_train_full[cname].nunique() < 10 and\n",
        "                    X_train_full[cname].dtype == \"object\"]\n",
        "low_cardinality"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQAbj6bjhv5R"
      },
      "source": [
        "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
        "high_cardinality = [cname for cname in X_train_full.columns if\n",
        "                    X_train_full[cname].nunique() > 10 and\n",
        "                    X_train_full[cname].dtype == \"object\"]\n",
        "high_cardinality"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ye0bnr2NGMk"
      },
      "source": [
        "##### We observe that all columns have high cardinality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA_QbbKyPSAx"
      },
      "source": [
        "# Columns that can be safely label encoded\n",
        "good_label_cols = [col for col in category_cols if\n",
        "                   set(X_train_full[col]) == set(X_valid_full[col])]\n",
        "\n",
        "\n",
        "# Problematic columns that will be dropped from the dataset\n",
        "bad_label_cols = list(set(category_cols) - set(good_label_cols))\n",
        "\n",
        "print('Categorical columns that will be label encoded:', good_label_cols)\n",
        "print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw06sULQPlAY"
      },
      "source": [
        "# Drop categorical columns that will not be encoded\n",
        "label_X_train = X_train_full.drop(bad_label_cols, axis=1)\n",
        "label_X_valid = X_valid_full.drop(bad_label_cols, axis=1)\n",
        "label_X_test = X_test_full.drop(bad_label_cols, axis=1)\n",
        "\n",
        "# print dataframe for inspection\n",
        "label_X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06IReH3NbH0C"
      },
      "source": [
        "# Apply label encoder to each column with categorical data\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "for col in good_label_cols:\n",
        "    label_X_train[col] = label_encoder.fit_transform(X_train_full[col])\n",
        "    label_X_valid[col] = label_encoder.transform(X_valid_full[col])\n",
        "    label_X_test[col] = label_encoder.transform(X_test_full[col])\n",
        "\n",
        "label_X_train.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4576ya_MO2nU"
      },
      "source": [
        "# Select numerical columns\n",
        "numerical_cols = [cname for cname in label_X_train.columns if\n",
        "                label_X_train[cname].dtype in ['int64', 'float64']]\n",
        "\n",
        "# Keep selected columns only\n",
        "categorical_cols = good_label_cols\n",
        "my_cols = categorical_cols + numerical_cols  # sometime use category_cols instead of good_label_cols for one-hot encoding\n",
        "X_train = label_X_train[my_cols].copy()\n",
        "X_valid = label_X_valid[my_cols].copy()\n",
        "X_test = label_X_test[my_cols].copy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8aryAxNewrj"
      },
      "source": [
        "# Define Preprocessing Steps and import dependecies\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "\n",
        "# Preprocessing for numerical data\n",
        "numerical_transformer = SimpleImputer(strategy='mean')\n",
        "#numerical_transformer = KNNImputer(n_neighbors=3)\n",
        "#numerical_transformer = Pipeline(steps=[('imputer',SimpleImputer() ),('scaler', StandardScaler())])\n",
        "\n",
        "# Preprocessing for categorical data\n",
        "categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent'))])\n",
        "#categorical_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder())])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sufb0JL5sYDe"
      },
      "source": [
        "### Model using Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38Nl-6S8RlwQ"
      },
      "source": [
        "# Step 1 : Bundle preprocessing for numerical and categorical data\n",
        "preprocessor = ColumnTransformer(transformers=[ ('num', numerical_transformer, numerical_cols),\n",
        "                ('cat', categorical_transformer, categorical_cols) ])\n",
        "\n",
        "# Step 2: Define the Model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model_1 = DecisionTreeClassifier(random_state=0)\n",
        "\n",
        "# Step 3: Create and Evaluate the Pipeline\n",
        "from sklearn.metrics import accuracy_score,average_precision_score,f1_score\n",
        "\n",
        "# Bundle preprocessing and modeling code in a pipeline\n",
        "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model_1)])\n",
        "\n",
        "# Preprocessing of training data, fit model\n",
        "# implement under_sampling to account for unbalanced data\n",
        "#from imblearn.under_sampling import NearMiss\n",
        "\n",
        "#rm = NearMiss(random_state = 1)\n",
        "#label_X_train_res, y_train_res = rm.fit_sample(label_X_train,y_train)\n",
        "my_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Preprocessing of validation data, get predictions\n",
        "preds = my_pipeline.predict(X_valid)\n",
        "\n",
        "# Evaluate the model\n",
        "score = accuracy_score(y_valid, preds)\n",
        "avg_precisionScore = average_precision_score(y_valid, preds)\n",
        "\n",
        "print('Accuracy Score:', score)\n",
        "print('Average Precision Score:', avg_precisionScore)\n",
        "print(\"The macro averaged f1_score is :\", f1_score(y_valid, preds, average='macro'))\n",
        "print(\"The mairo averaged f1_score is :\", f1_score(y_valid, preds, average='micro'))\n",
        "print(\"The weighted averaged f1_score is :\", f1_score(y_valid, preds, average='weighted'))\n",
        "print(\"The  f1_score is :\", f1_score(y_valid, preds, average='macro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCtNiLMskTTz"
      },
      "source": [
        "# Cross validation Score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(my_pipeline, X_train, y_train, cv=3)\n",
        "\n",
        "# Print the mean score and 95% confidence interval\n",
        "print(\"Accuracy: %0.2f (+/- %0.5f)\" % (scores.mean(), scores.std() * 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1xMwAiOn1sF"
      },
      "source": [
        "# print classification report (confusion matrix)\n",
        "from sklearn.metrics import classification_report\n",
        "print (classification_report(y_valid, preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp8X4bygpc-k"
      },
      "source": [
        "# Use imbalance library to calculate sensitiivty score  of model\n",
        "from imblearn.metrics import sensitivity_score\n",
        "print(sensitivity_score(y_valid, preds, average='macro'))\n",
        "print(sensitivity_score(y_valid, preds, average='micro'))\n",
        "print(sensitivity_score(y_valid, preds, average='weighted'))\n",
        "print(sensitivity_score(y_valid, preds, average=None))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwq3Cw27qiHM"
      },
      "source": [
        "# Use imbalance library to calculate specificity score  of model\n",
        "from imblearn.metrics import specificity_score\n",
        "print(specificity_score(y_valid, preds, average='macro'))\n",
        "print(specificity_score(y_valid, preds, average='micro'))\n",
        "print(specificity_score(y_valid, preds, average='weighted'))\n",
        "print(specificity_score(y_valid, preds, average=None))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WupO8eGFUfkd"
      },
      "source": [
        "# Preprocessing of test data, fit model\n",
        "preds_test = my_pipeline.predict(X_test)\n",
        "\n",
        "# Save test predictions to file\n",
        "output = pd.DataFrame({'Id': X_test.index,'CANCELLED': preds_test})\n",
        "output.to_csv('submission_DT.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOB8JJhXsK5C"
      },
      "source": [
        "### Model using Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT3Z3nqtsQAe"
      },
      "source": [
        "# Step 1 : Bundle preprocessing for numerical and categorical data\n",
        "preprocessor = ColumnTransformer(transformers=[ ('num', numerical_transformer, numerical_cols),\n",
        "                ('cat', categorical_transformer, categorical_cols) ])\n",
        "\n",
        "# Step 2: Define the Model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model_2 = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "\n",
        "# Step 3: Create and Evaluate the Pipeline\n",
        "from sklearn.metrics import accuracy_score,average_precision_score\n",
        "\n",
        "# Bundle preprocessing and modeling code in a pipeline\n",
        "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model_2)])\n",
        "\n",
        "# Preprocessing of training data, fit model\n",
        "my_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Preprocessing of validation data, get predictions\n",
        "preds = my_pipeline.predict(X_valid)\n",
        "\n",
        "# Evaluate the model\n",
        "score = accuracy_score(y_valid, preds)\n",
        "avg_precisionScore = average_precision_score(y_valid, preds)\n",
        "\n",
        "print('Accuracy Score:', score)\n",
        "print('Average Precision Score:', avg_precisionScore)\n",
        "print(\"The macro averaged f1_score is :\", f1_score(y_valid, preds, average='macro'))\n",
        "print(\"The mairo averaged f1_score is :\", f1_score(y_valid, preds, average='micro'))\n",
        "print(\"The weighted averaged f1_score is :\", f1_score(y_valid, preds, average='weighted'))\n",
        "print(\"The  f1_score is :\", f1_score(y_valid, preds, average='macro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3V6YHFh7s4zv"
      },
      "source": [
        "# Cross validation Score\n",
        "##scores = cross_val_score(my_pipeline, X_train, y_train, cv=5)\n",
        "\n",
        "# Print the mean score and 95% confidence interval\n",
        "\n",
        "#print(\"Accuracy: %0.2f (+/- %0.5f)\" % (scores.mean(), scores.std() * 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vixznA5ytCDX"
      },
      "source": [
        "# print classification report (confusion matrix)\n",
        "from sklearn.metrics import classification_report\n",
        "print (classification_report(y_valid, preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r-C931otJ9K"
      },
      "source": [
        "# Use imbalance library to calculate sensitiivty score  of model\n",
        "from imblearn.metrics import sensitivity_score\n",
        "print(sensitivity_score(y_valid, preds, average='macro'))\n",
        "print(sensitivity_score(y_valid, preds, average='micro'))\n",
        "print(sensitivity_score(y_valid, preds, average='weighted'))\n",
        "print(sensitivity_score(y_valid, preds, average=None))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fidLdTeItKxe"
      },
      "source": [
        "# Use imbalance library to calculate specificity score  of model\n",
        "from imblearn.metrics import specificity_score\n",
        "print(specificity_score(y_valid, preds, average='macro'))\n",
        "print(specificity_score(y_valid, preds, average='micro'))\n",
        "print(specificity_score(y_valid, preds, average='weighted'))\n",
        "print(specificity_score(y_valid, preds, average=None))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBL5cybwtQ7n"
      },
      "source": [
        "# Preprocessing of test data, fit model\n",
        "preds_test = my_pipeline.predict(X_test)\n",
        "\n",
        "# Save test predictions to file\n",
        "output = pd.DataFrame({'Id': X_test.index,'CANCELLED': preds_test})\n",
        "output.to_csv('submission_RF.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49diYT5X4g61"
      },
      "source": [
        "### Model using SVM Classifier (radial Kerenel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKslXORZ5ku_"
      },
      "source": [
        "### Model using Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FmkhQXN5rYW"
      },
      "source": [
        "# Step 1 : Bundle preprocessing for numerical and categorical data\n",
        "numerical_transformer = Pipeline(steps=[('imputer',SimpleImputer() ),('scaler', StandardScaler())])\n",
        "preprocessor = ColumnTransformer(transformers=[ ('num', numerical_transformer, numerical_cols),\n",
        "                ('cat', categorical_transformer, categorical_cols) ])\n",
        "\n",
        "# Step 2: Define the Model\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "model_5 = GaussianNB()\n",
        "\n",
        "\n",
        "# Step 3: Create and Evaluate the Pipeline\n",
        "from sklearn.metrics import accuracy_score,average_precision_score\n",
        "\n",
        "# Bundle preprocessing and modeling code in a pipeline\n",
        "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model_5)])\n",
        "\n",
        "# Preprocessing of training data, fit model\n",
        "my_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Preprocessing of validation data, get predictions\n",
        "preds = my_pipeline.predict(X_valid)\n",
        "\n",
        "# Evaluate the model\n",
        "score = accuracy_score(y_valid, preds)\n",
        "avg_precisionScore = average_precision_score(y_valid, preds)\n",
        "\n",
        "print('Accuracy Score:', score)\n",
        "print('Average Precision Score:', avg_precisionScore)\n",
        "print(\"The macro averaged f1_score is :\", f1_score(y_valid, preds, average='macro'))\n",
        "print(\"The mairo averaged f1_score is :\", f1_score(y_valid, preds, average='micro'))\n",
        "print(\"The weighted averaged f1_score is :\", f1_score(y_valid, preds, average='weighted'))\n",
        "print(\"The  f1_score is :\", f1_score(y_valid, preds, average='macro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIe8kVwG6D3F"
      },
      "source": [
        "# Cross validation Score\n",
        "scores = cross_val_score(my_pipeline, X_train, y_train, cv=5)\n",
        "# Print the mean score and 95% confidence interval\n",
        "print(\"Accuracy: %0.2f (+/- %0.5f)\" % (scores.mean(), scores.std() * 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4-gUA8W6ES9"
      },
      "source": [
        "# print classification report (confusion matrix)\n",
        "from sklearn.metrics import classification_report\n",
        "print (classification_report(y_valid, preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drr6ryo46EgT"
      },
      "source": [
        "# Use imbalance library to calculate sensitiivty score  of model\n",
        "from imblearn.metrics import sensitivity_score\n",
        "print(sensitivity_score(y_valid, preds, average='macro'))\n",
        "print(sensitivity_score(y_valid, preds, average='micro'))\n",
        "print(sensitivity_score(y_valid, preds, average='weighted'))\n",
        "print(sensitivity_score(y_valid, preds, average=None))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDypGbzx6RDT"
      },
      "source": [
        "# Use imbalance library to calculate specificity score  of model\n",
        "from imblearn.metrics import specificity_score\n",
        "print(specificity_score(y_valid, preds, average='macro'))\n",
        "print(specificity_score(y_valid, preds, average='micro'))\n",
        "print(specificity_score(y_valid, preds, average='weighted'))\n",
        "print(specificity_score(y_valid, preds, average=None))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alQ3OUBV6VC0"
      },
      "source": [
        "# Preprocessing of test data, fit model\n",
        "preds_test = my_pipeline.predict(X_test)\n",
        "\n",
        "# Save test predictions to file\n",
        "output = pd.DataFrame({'Id': X_test.index,'CANCELLED': preds_test})\n",
        "output.to_csv('submission_NB.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}